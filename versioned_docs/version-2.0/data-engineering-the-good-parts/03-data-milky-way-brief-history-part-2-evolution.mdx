---
sidebar_position: 3
authors: [plaosunthara, tklae]
minutesToComplete: 140
---

# Data Milky Way: A Brief History (Part 2) - Evolution

<div style={{textAlign: 'center'}}>

<figure class="video-container">
  <iframe
    width="560"
    height="315"
    src="https://www.youtube.com/embed/MxSwJQvWKuw"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen
  ></iframe>
</figure>

</div>

## 1980s: Databases & Data Warehouses

Since the 1980s up until the mid-late 2000s, businesses still stored both [OLTP and OLAP](https://www.youtube.com/watch?v=-v3PhEtOuxw&ab_channel=DataCamp) data in relational databases (RDBMS)

- e.g. Oracle, Microsoft SQL Server, MySQL, PostgreSQL, etc.
- OLAP-oriented databases for storing large amounts of data were called **Data Warehouses**

- Relational databases were notoriously difficult to distribute/scale out
- As more data came in, businesses often opted to scale up ðŸ’° ([scale up vs scale out](https://hackernoon.com/database-scaling-horizontal-and-vertical-scaling-85edd2fd9944))
  - Greater investment risk, difficult to plan capacity, expensive
  - Restrictive limits to CPU & memory for a single server
  - Nowadays, modern Massively Parallel Processing (MPP) warehouses can provide scale out capabilities

However, Data Warehouses still require careful **upfront** planning

- Schema and layout need to be decided beforehand
- Query patterns/use-cases needed to be preempted
- Separation of storage and compute often a weakness (perhaps except for modern warehouses such as
  Google BigQuery and Snowflake)

<div style={{textAlign: 'center'}}>

![scaling.png](./assets/scaling.png)

</div>

## Data Warehouses

![data-warehouses.png](./assets/data-warehouses.png)

Pros

- Good for Business Intelligence (BI) applications (structured data, so long as it isn't too massive)

Cons

- **Limited support** for advanced analytics & machine learning workloads
- **Limited support** for non-tabular, unstructured data (e.g. free text, images)
- Proprietary systems with only a SQL interface

Because of the limitations around flexibility and scaling, a new technology emerged in the early 2000s.

## Early 2000s: Hadoop Hype Train

Arrival of On-Prem Data Lakes
(2004) Google published the [MapReduce](https://www.youtube.com/watch?v=s8EPQpgpWVE&ab_channel=CBTNuggets) whitepaper, inspiring the Apache Hadoop project

- enabled **(on-prem)** distributed data processing on **commodity (cheap) hardware**
- businesses began throwing data into their Hadoop clusters!

<div style={{textAlign: 'center'}}>

![joy-of-hadoop.png](./assets/joy-of-hadoop.png)

</div>

<div style={{textAlign: 'center'}}>

![hadoop-network.png](./assets/hadoop-network.png)

**Bonus content**: [What you need to know about Hadoop](https://www.oreilly.com/content/hadoop-what-you-need-to-know/) (Very detailed and not required for the remainder of the course!)

</div>

## The Hadoop Disillusionment

Several years later, people became [disillusioned about Hadoop](https://www.datanami.com/2017/03/13/hadoop-failed-us-tech-experts-say/#:~:text=The%20Hadoop%20dream%20of%20unifying,find%20a%20happy%20Hadoop%20customer.):

<div style={{textAlign: 'center'}}>

![hadoop-has-failed-us.png](./assets/hadoop-has-failed-us.png)

</div>

Working with distributed computing can be extremely challenging with respect to the learning curve, especially for business analysts. Instead of working with SQL, they'd have to learn Java libraries and frameworks related to MapReduce.

Excerpt:

:::info

Hadoop is great if you're a data scientist who knows how to code in MapReduce or Pig, Johnson says, but as you go higher up the stack, the **abstraction layers have mostly failed to deliver on the promise of enabling business analystics to get at the data.**

:::

In addition to the learning curve, the performance was surprisingly slow for BI users. They would have expected that compared to the warehouse, they would have had performance gains but in fact the queries were running a lot slower on hadoop than on the traditional data warehouses.

Excerpt:

:::info

"At the Hive layer, it's kind of OK. But people think they're going to use Hadoop for data warehouse...**are pretty surprised that this hot new technology is 10x slower than what they're using before**," Johnson says.
:::

This disillusionment led to the sunsetting of Hadoop projects, including at Google (who were the inventors of the entire ecossystem).

<div style={{textAlign: 'center'}}>

![urs-hoelzle.png](./assets/urs-hoelzle.png)

Senior Vice President of Technical Infrastructure, Google

</div>

While data locality + coupling storage and compute in Hadoop clusters was a decent idea for data throughputâ€¦

- Businesses were forced to increase **both** CPU & Disk when they often only needed to
  scale up just one or the other
- Youâ€™d have to pay for more CPU just to store inactive, rarely-utilized data, what a waste!
- Storing and replicating data on HDFS (Hadoop Distributed File System) was **expensive**
  and difficult to maintain
- Query performance was **lackluster** and other beneficial properties of RDBMS were gone

## Latency

Latency nightmare when only using disk and network (i.e. MapReduce). Let's get a feel for it relative to "humanized" time.

|              Type              |    Time     | Time (humanized form) |
| :----------------------------: | :---------: | :-------------------: |
|          1 CPU Cycle           |   0.3 ns    |          1 s          |
|      Level 1 Cache Access      |   0.9 ns    |          3 s          |
|      Level 2 Cache Access      |   2.8 ns    |          9 s          |
|      Level 3 Cache Access      |   12.9 ns   |         43 s          |
|       Main Memory Access       |   120 ns    |         6 min         |
|      Solid-state disk I/O      | 50 - 150 Âµs |      2 - 6 days       |
|      Rotational disk I/O       |  1 - 10 ms  |     1 - 12 months     |
|      Internet: SF to NYC       |    40 ms    |        4 years        |
|   Internet: SF to Australia    |   183 ms    |       19 years        |
|    OS Virtualization reboot    |     4 s     |       23 years        |
|     SCSI command time-out      |    30 s     |      3000 years       |
| Hardware Virtualization reboot |    40 s     |      4000 years       |
|     Physical System reboot     |     5 m     |      32 millenia      |

## Latency matters in Hadoop

Hadoop is all about fault tolerance and simple api of map and reduce steps. But all that fault tolerance in Hadoop comes at a very huge cost:

1. To recover from potential failures, Hadoop MapReduce writes intermediate data between Map and Reduce steps to the disk for every job.
2. The MapReduce implementation in Hadoop, by design, shuffles the data between map and reduce steps, which is expensive.

Now looking at the above table, we know that writing to the disk and network communications are slow and expensive and quite naturally increases latency. Because of the inherent behavior of MapReduce (constant data transmissions and persisting), attention must be paid to the amount of data that gets transmitted to manage both speed and costs of queries.

## Spark is pre-built to reduce latency

Spark also provides fault tolerance like Hadoop but with significantly reduced latency in the following way :

1. It keeps all data immutable and in memory, thus avoiding disk writes, by keeping data in memory.
2. It provides Scala like chains of operations (called transformations) and keeps track of it in its light weight client side process called as Spark Driver.
3. In the event of a node failure, it finds a new node, copies the partition data on that node and just simply plays the tracked operations in step 2 above.

Additionally Spark provides rich APIs in Scala, Python, Java and R. Lastly, unlike hadoop, it makes data shuffle optional by making it necessary only when using certain spark operations (formally called wide transformations in Spark) like joins, groupBy etc.

**Optional content**: 24 minute video version of Latency in Big Data (demonstrates the massive impact of using memory vs disk vs network in Big Data):

<div style={{textAlign: 'center'}}>

<figure class="video-container">
  <iframe
    width="560"
    height="315"
    src="https://www.youtube.com/embed/DXq5MOYGK1U"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen
  ></iframe>
</figure>

</div>

## Cloud Revolution

Cloud Revolution: [Why object storage wins over Hadoop-based storage](https://www.ibm.com/cloud/blog/cutting-cord-separating-data-compute-data-lake-object-storage)

- To scale cost-effectively, we need to really separate compute and storage
  - e.g. simply provision more CPU-intensive clusters only when needed, while leaving storage the same
- As analytics and AI began to involve images, audio, unstructured data:
  - Cloud Data Lakes (often based on object storage) became the ideal storage solution

Time for **unified analytics/query engines** such as **Spark** and **Presto** to shine ðŸ’«

- [Spark](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch01.html) & [Presto](https://prestodb.io/overview.html)
  - Both engines excel when running analytical queries against data stored on Object Storage (e.g. Amazon S3, Azure Blob Storage)
  - Both engines take advantage of **both memory and disk** (unlike Hadoop MapReduce which read/writes data via disk only)

**Spark** (much more on this later) is **extremely popular for programmatic (Python/Scala/Java/R) use-cases** but can also support SQL queries.

**Presto** is a popular choice for **ad-hoc interactive SQL queries** (e.g. AWS Athena is a serverless offering based on Presto).

Many companies (especially tech giants) can even often have both Spark and Presto/Athena in their stack:

<div style={{textAlign: 'center'}}>

![presto-spark-netflix-platform.png](./assets/presto-spark-netflix-platform.png)

</div>

## Unified Analytics Engines

Both Spark and Presto can be considered **Unified Analytics Engines**, meaning they can cover most of your data needs
from ingestion from different sources over transformation to visualisations. What's special is that this multi-purpose
approach enables you to adjust the technologies to your needs (and the arrangement of your data) and not vice versa.

<div style={{textAlign: 'center'}}>

![spark.png](./assets/spark.png)
![presto-cluster.png](./assets/presto-cluster.png)

</div>

### Mission Accomplished?

While being able to directly query your cloud object storage (e.g. S3, Azure Data Lake Storage)
with big data engines such as Spark and Presto moved the field in a great direction, some missing pieces still remained.
Let's have a look at the setup of traditional data lakes versus more versatile solutions.

## 2000s - 2010s: Traditional Data Lakes

<div style={{textAlign: 'center'}}>

![traditional-data-lakes.png](./assets/traditional-data-lakes.png)

</div>

**Pros**

- Extremely cheap and scalable
- Open, arbitrary data formats and big ecosystem
- Supports ML

**Cons**

- Some BI workloads still not snappy enough
- Complex data quality problems
- No data management layer
- Difficult for GDPR compliance

## Can we get the best of both worlds?

The goal in this space is to get rid of the current shortcomings listed above. We want to continue to use cheap and
scalable computing resources and a wide ecosystem while being able to tackle more advanced problems, opening up
data related problems to entire companies while maintaining appropriate data security and access policies.

One approach is the so-called **Data Lakehouse** which weâ€™ll revisit in the second week of the course.

<div style={{textAlign: 'center'}}>

![data-lakehouse.png](./assets/data-lakehouse.png)

</div>

Other challengers in this area are [Delta Lake](https://delta.io/), [Dremio](https://www.dremio.com/) or
[Databricks Photon](https://databricks.com/product/photon).
Weâ€™ll also cover how Lakehouse can serve as the technological foundation to support philosophies such as Data Mesh near
the end of this course.
