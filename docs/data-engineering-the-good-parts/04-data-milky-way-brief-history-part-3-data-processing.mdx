---
sidebar_position: 4
authors: [plaosunthara, tklae]
minutesToComplete: 40
---

# Data Milky Way: Brief History (Part 3) - Data Processing

<div style={{textAlign: 'center'}}>

<figure class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Uc-Wtem-lyw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>

</div>


## Evolution of Data Processing
<div style={{textAlign: 'center'}}>

![map-reduce-processing.png](./assets/map-reduce-processing.png)
![data-processing-vision.png](./assets/data-processing-vision.png)

</div>

* Recap: [Why move from Hadoop to Spark + Object Storage](https://www.xplenty.com/blog/apache-spark-vs-hadoop-mapreduce/) (~20 minute read)
* **Bonus Content, not necessary for the progression in this course**:
    * [Intro to Metastores and Data Catalogs](https://lakefs.io/metadata-management-hive-metastore-vs-aws-glue/) (~10 minutes)
    * [Batch and Micro-Batch Streaming](https://www.upsolver.com/blog/batch-stream-a-cheat-sheet) (~20 minutes)
    * [Continuous Processing](https://hazelcast.com/glossary/stream-processing/) (~10 minutes)
    * [One syntax to rule them all?](https://beam.apache.org/) (~2 minutes)
        * Apache Beam is based on the [Dataflow model introduced by Google](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43864.pdf) (scientific paper, will take a while to digest)
        * Aims to unify the semantics of batch & streaming processing across engines (Flink, Spark, etc.)
        * You donâ€™t necessarily need streaming, let alone Beam! Many teams simply choose either Spark Structured Streaming or Flink without Beam.

Evaluate your own projectâ€™s needs.
We will cover streaming in the second half of the course, so more material will be provided then.



## Orchestration Core Concepts
But how do we make our pipeline **flow**? ðŸŒŠ

* Data Engineering workflows often involve transforming and transferring data from one place to another.
* We want to combine data from different locations, and we want to do this in a way that is **reproducible** and **scalable** when
  there are updates to the data or to our workflows.
* Workflows in real-life have multiple steps and stages. We want to be able to **orchestrate** these steps and stages and
  **monitor** the progress of our workflows.
* Sometimes, everything might work fine with just CRON jobs.
* But other times, you might want to control the state transitions of these steps:
  * e.g. if Step A doesnâ€™t run properly, donâ€™t run Step B because the data could be corrupt, instead run Step C.
  * Once again, the concept of [Directed Acyclic Graphs (DAGs)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) can come to our rescue.


* **Bonus Content**: [Apache Airflow](https://www.youtube.com/watch?v=XD7euLOzKbs&ab_channel=SFPython) (32 minute video) is one nice way of setting up DAGs to orchestrate jobs ðŸŒˆ
  * Note: Airflow is primarily designed as a task orchestration tool.
  * You can trigger tasks on the Airflow cluster itself or on remote targets (e.g. AWS Fargate, Databricks, etc.).
  * NOT designed for transferring large amounts of actual data.
  * [Reference Documentation](https://airflow.apache.org/docs/apache-airflow/1.10.1/#beyond-the-horizon)
  * [Play around with Airflow locally](https://github.com/kelseymok/airflow/) (very optional!)



## Practical Data Workloads

<div style={{textAlign: 'center'}}>

![big-data-sword.png](./assets/big-data-sword.png)

Weâ€™re here to teach you big data skills, but in reality...

</div>

### Single-Node vs. Cluster
Not everything is Big Data! You donâ€™t always need Spark!
[Sometimes Pandas deployed on a single node function/container is just fine!](https://www.indellient.com/blog/a-journey-from-pandas-to-spark-data-frames/).

### Batch vs Streaming
[Streaming isnâ€™t always the solution](https://www.section.io/engineering-education/batch-processing-vs-stream-processing/)! (Optional reading, ~10 minutes)

### Orchestration options
Here, it's just useful to know a few of the names but going into detail is not necessary for the course.

**DAG-based approaches:**
* [Apache Airflow](https://airflow.apache.org/)
* [Databricks Jobs Orchestration](https://databricks.com/blog/2021/07/13/announcement-orchestrating-multiple-tasks-with-databricks-jobs-public-preview.html)
* [Dagster](https://dagster.io/)

**Event-Driven + Declarative**
* [Databricks Auto Loader](https://databricks.com/discover/demos/delta-lake-data-integration-demo-auto-loader-and-copy-into)
* [Delta Live Tables](https://databricks.com/discover/demos/delta-live-tables-demo)

**Other triggers:**
* [AWS Lambda](https://aws.amazon.com/lambda/)
* [Glue Triggers](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html)



